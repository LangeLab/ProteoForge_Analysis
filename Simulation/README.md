# Simulation Framework for ProteoForge Evaluation

## Overview

This directory contains a simulation-based framework to evaluate proteoform detection methods under controlled scenarios. It complements the real-data benchmarks by generating synthetic datasets with known perturbations, then running COPF, PeCorA, and ProteoForge to assess performance at both peptide identification and protein-level grouping.

The simulation suite provides:

- Synthetic datasets across multiple scenarios with ground-truth labels
- Reproducible pipelines for method execution (R and Python)
- Standard evaluation metrics and consolidated result artifacts
- Figure assembly notebooks used in the manuscript

## Contents

- `00-DemoFigure.ipynb` — minimal walkthrough using demo data (manuscript Figure 1)
- `01-SimulatedDatasets.ipynb` — describes scenario generation and data characteristics
- `02-runCOPF.R` — runs COPF across simulation scenarios
- `03-runPeCorA.R` — runs PeCorA across simulation scenarios
- `04-runProteoForge.py` — runs ProteoForge across simulation scenarios
- `05-IdentificationBenchmark.ipynb` — peptide-level accuracy evaluation
- `06-GroupingBenchmark.ipynb` — protein-level grouping accuracy evaluation
- `07-FigureAssembly.ipynb` — assembles figures from results (manuscript Figure 2)
- `sims.py` — Python helpers for simulation data handling
- `data/` — prepared synthetic data and results (generated by notebooks/scripts)
- `figures/` — figures organized by scenario and analysis stage (generated by notebooks)

## Inputs and Outputs

- Inputs:

    - Prepared synthetic datasets under `data/prepared/` (e.g., `demo_data.csv`) and per-scenario folders (`data/Sim1/`, `data/Sim2/`, `data/Sim3/`, `data/Sim4/`).
    - Each scenario includes `*_InputData.feather` files annotated with ground-truth labels.

- Outputs:

    - Method results written as feather/CSV files per scenario (e.g., `*_ProteoForge_ResultData.feather`, `*_PeCorA_ResultData.feather`, `*_COPF_ResultData.feather`).
    - Consolidated performance summaries (e.g., `4_Sim*_Grouping_PerformanceData.*`, `4_Sim*_PeptideIdentification_PerformanceData.*`).
    - Figures under `figures/` organized by scenario and step (e.g., `figures/Sim1/`, `figures/00-FigureAssembly/`).

Note: Large datasets and figures are not tracked by git; they will be generated by the scripts/notebooks and stored under `data/` and `figures/` respectively.

## Run Steps

Execution order for full simulation analysis:

1. `01-SimulatedDatasets.ipynb` — generate or describe the simulated datasets and scenarios
2. `02-runCOPF.R` — run COPF across all scenarios
3. `03-runPeCorA.R` — run PeCorA across all scenarios
4. `04-runProteoForge.py` — run ProteoForge across all scenarios
5. `05-IdentificationBenchmark.ipynb` — evaluate peptide-level identification accuracy
6. `06-GroupingBenchmark.ipynb` — evaluate protein-level grouping accuracy

Each notebook/script documents its required inputs and expected outputs. Ensure that the inputs exist in `data/prepared/` and per-scenario directories (`data/Sim1/` through `data/Sim4/`) before running.

## Scenarios and Metrics

Scenarios typically include controlled patterns such as `twoPep`, `halfPep`, `halfPlusPep`, and `randomPep`, each with complete/imputed variations. Ground truth labels enable standard metric calculation:

- Peptide identification: TPR (Recall), FPR, Precision, F1, MCC, ROC-AUC, PR-AUC
- Proteoform grouping (protein-level): metrics based on predicted group membership vs. ground truth

Method scope alignment:

- COPF — designed for protein-level grouping
- PeCorA — designed for peptide-level discordance identification
- ProteoForge — supports both identification and grouping
